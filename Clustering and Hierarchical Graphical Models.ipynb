{
 "metadata": {
  "name": "",
  "signature": "sha256:35392eacaca25811af5705c36c487f5fc0086659c5981ab1df289c44b1da2b7e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\newcommand{\\brac}   [1] {{ \\left( #1 \\right) }}\n",
      "\\newcommand{\\sbrac}  [1] {{ \\left[ #1 \\right] }}\n",
      "\\newcommand{\\cbrac}  [1] {{ \\left\\{ #1 \\right\\} }}\n",
      "\\newcommand{\\abrac}  [1] {{ \\langle #1 \\rangle }}\n",
      "\\newcommand{\\real}   [1] {{\\mathbb{R}^{#1}}} \n",
      "\\newcommand{\\lnorm}  [1] {{\\ell_{#1}}}    \n",
      "\\newcommand{\\norm}   [2] {{\\|{#1}\\|_{#2}}} \n",
      "\\newcommand{\\abs}    [1] {{\\lvert{#1}\\rvert}}     \n",
      "\\newcommand{\\nullsp} [1] {{\\mathrm{Null}\\!\\brac{#1}}}\n",
      "\\newcommand{\\bigo}   [1] {{\\mathcal{O}\\!\\brac{#1}}} \n",
      "\\newcommand{\\trace}  [1] {{\\mathrm{Tr}\\!\\brac{#1}}}\n",
      "\\newcommand{\\argmax} [1] {{\\underset{#1}{\\arg\\max~}}}\n",
      "\\newcommand{\\argmin} [1] {{\\underset{#1}{\\arg\\min~}}}\n",
      "\\newcommand{\\indic}  [1] {{{\\mathbf{1}\\!\\sbrac{#1}}}}\n",
      "\\newcommand{\\digam}  [1] {{\\Psi\\!\\brac{#1}}}\n",
      "\\newcommand{\\gamfn}  [1] {{\\Gamma\\!\\brac{#1}}}\n",
      "\\newcommand{\\gamfnD} [1] {{\\Gamma_D\\!\\brac{#1}}}\n",
      "\\newcommand{\\transpose}  {{^{\\top}\\!}}\t\t\n",
      "\\newcommand{\\deter}  [1] {{\\left|{#1}\\right|}}\n",
      "\\newcommand{\\prob}   [1] {{p\\brac{#1}}}  \n",
      "\\newcommand{\\probC}  [2] {{p\\brac{#1|#2}}}\n",
      "\\newcommand{\\qrob}   [1] {{q\\!\\brac{#1}}} \n",
      "\\newcommand{\\qrobC}  [2] {{q\\!\\brac{#1|#2}}} \n",
      "\\newcommand{\\gaus}   [1] {{\\mathcal{N}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\gausC}  [2] {{\\mathcal{N}\\!\\brac{#1|#2}}}\n",
      "\\newcommand{\\gam}    [1] {{\\mathrm{Gamma}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\gamC}   [2] {{\\mathrm{Gamma}\\!\\brac{#1|#2}}}\n",
      "\\newcommand{\\betad}  [1] {{\\mathrm{Beta}\\!\\brac{#1}}}\n",
      "\\newcommand{\\betaC}  [2] {{\\mathrm{Beta}({#1}|{#2})}}\n",
      "\\newcommand{\\dir}    [1] {{\\mathrm{Dir}\\brac{#1}}}\t\n",
      "\\newcommand{\\dirC}   [2] {{\\mathrm{Dir}\\brac{#1|#2}}}\n",
      "\\newcommand{\\DP}     [1] {{\\mathrm{DP}\\!\\brac{#1}}}\n",
      "\\newcommand{\\SB}     [1] {{\\mathrm{SB}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\wish}   [1] {{\\mathcal{W}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\wishC}  [2] {{\\mathcal{W}\\!\\brac{#1|#2}}}\t\n",
      "\\newcommand{\\categ}  [1] {{\\mathrm{Categ}\\brac{#1}}}\n",
      "\\newcommand{\\categC} [2] {{\\mathrm{Categ}\\brac{#1|#2}}}\n",
      "\\newcommand{\\ncons}  [1] {{\\mathcal{Z}_{#1}}}\n",
      "\\newcommand{\\bigO}   [1] {{\\mathcal{O}({#1})}}\n",
      "\\newcommand{\\obsall}{\\mathbf{X}}\n",
      "\\newcommand{\\obsind}{\\mathbf{x}}\n",
      "\\newcommand{\\sobsind}{x}\n",
      "\\newcommand{\\iobsall}{\\mathbf{W}}\n",
      "\\newcommand{\\iobsind}{\\mathbf{w}}\n",
      "\\newcommand{\\obsbar}{\\bar{\\mathbf{x}}}\n",
      "\\newcommand{\\obscov}{\\bar{\\mathbf{S}}}\n",
      "\\newcommand{\\olaball}{\\mathbf{Z}}\n",
      "\\newcommand{\\olabgrp}{\\mathbf{z}}\n",
      "\\newcommand{\\olabind}{z}\n",
      "\\newcommand{\\ilaball}{\\mathbf{Y}}\n",
      "\\newcommand{\\ilabind}{y}\n",
      "\\newcommand{\\allparam}{\\boldsymbol\\Theta}\n",
      "\\newcommand{\\allhyper}{\\boldsymbol\\Xi}\n",
      "\\newcommand{\\mwgtall}{\\mathbf{B}}\n",
      "\\newcommand{\\mwgtind}{\\boldsymbol{\\beta}}\n",
      "\\newcommand{\\mwgtmix}{\\beta}\n",
      "\\newcommand{\\wgtall}{\\boldsymbol\\Pi}\n",
      "\\newcommand{\\wgtind}{\\boldsymbol\\pi}\n",
      "\\newcommand{\\wgtmix}{\\pi}\n",
      "\\newcommand{\\stkall}{\\mathbf{V}}\n",
      "\\newcommand{\\stkmix}{v}\n",
      "\\newcommand{\\expall}{\\boldsymbol\\Theta}\n",
      "\\newcommand{\\expmix}{\\theta}\n",
      "\\newcommand{\\hypexn}{\\eta}\n",
      "\\newcommand{\\hypexv}{\\boldsymbol\\nu}\n",
      "\\newcommand{\\gausmean}{\\boldsymbol\\mu}\n",
      "\\newcommand{\\gauscov}{\\boldsymbol\\Sigma}\n",
      "\\newcommand{\\hypgausm}{\\mathbf{m}}\n",
      "\\newcommand{\\hypgausb}{\\gamma}\n",
      "\\newcommand{\\hypgausW}{\\boldsymbol{\\Omega}}\n",
      "\\newcommand{\\hypgausp}{\\rho}\n",
      "\\newcommand{\\igausmean}{\\boldsymbol\\eta}\n",
      "\\newcommand{\\igauscov}{\\boldsymbol\\Psi}\n",
      "\\newcommand{\\ihypgausm}{\\mathbf{h}}\n",
      "\\newcommand{\\ihypgausb}{\\delta}\n",
      "\\newcommand{\\ihypgausW}{\\boldsymbol{\\Phi}}\n",
      "\\newcommand{\\ihypgausp}{\\xi}\n",
      "\\newcommand{\\dirall}{\\alpha}\n",
      "\\newcommand{\\dirmix}{\\alpha}\n",
      "\\newcommand{\\dircall}{\\theta}\n",
      "\\newcommand{\\dircmix}{\\theta}\n",
      "\\newcommand{\\gdaall}{a}\n",
      "\\newcommand{\\gdball}{b}\n",
      "\\newcommand{\\gdamix}{a}\n",
      "\\newcommand{\\gdbmix}{b}\n",
      "\\newcommand{\\Cwidthi} {\\ensuremath{C_{w,i}}}\n",
      "\\newcommand{\\Cwidths} {\\ensuremath{C_{w,s}}}\n",
      "\\newcommand{\\ICAdic}  {\\ensuremath{\\mathbf{D}}}\n",
      "\\newcommand{\\ICAdicp} {\\ensuremath{\\mathbf{D}^+}}\n",
      "\\newcommand{\\ICAresp} {\\ensuremath{\\mathbf{r}}}\n",
      "\\newcommand{\\Segment} {\\ensuremath{S_{jin}}}\n",
      "\\newcommand{\\dimim}   {\\ensuremath{D_\\mathrm{im}}}\n",
      "\\newcommand{\\dimseg}  {\\ensuremath{D_\\mathrm{seg}}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Clustering\n",
      "Clustering is one of the oldest data exploration methods. The objective is for an algorithm to discover sets of *similar* points, or observations, within a larger dataset. These sets are called *clusters*.  Similarity is almost always characterised by some distance function between observations, such as Euclidean $\\ell_2$. Some of the more simple algorithms require the number of clusters to be specified in advance, while others can also infer this from the data, usually given other assumptions. K-means was one of the first and still most popular algorithms designed for this task."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##K-means\n",
      "\n",
      "The objective of K-means clustering is to find $K$ clusters of observations\n",
      "within a dataset of $N$ observations, $\\obsall = \\cbrac{\\obsind_n}^N_{n=1}$, where $\\obsind_n \\in \\real{D}$. These clusters are characterised by their means, $\\mathbf{M} =\n",
      "\\cbrac{\\gausmean_k}^K_{k=1}$ where $\\gausmean_k \\in \\real{D}$. Each observation\n",
      "is assigned to a cluster mean using an integer label $\\olabind_n \\in \\cbrac{1,\\ldots,K}$,\n",
      "and $\\olaball = \\cbrac{\\olabind_n}^N_{n=1}$. The objective of K-means is to\n",
      "minimise the square loss, or reconstruction error,\n",
      "\\begin{equation}\n",
      "   \\min_{\\mathbf{M},\\olaball} \\sum^N_{n=1} \\sum^K_{k=1} \\indic{\\olabind_n = k}\n",
      "      \\norm{\\obsind_n - \\gausmean_k}{2}^2.\n",
      "\\end{equation}\n",
      "Here $\\indic{\\cdot}$ is an indicator function that evaluates to 1 when the\n",
      "condition in the brackets is true, and 0 otherwise. $\\norm{\\cdot}{2}$ is an\n",
      "$\\ell_2$ norm, or Euclidean distance. This is solved with two simple alternating\n",
      "steps. The first is the assignment step;\n",
      "\\begin{equation}\n",
      "   \\olabind_n = \\argmin{k} \\norm{\\obsind_n - \\gausmean_k}{2}^2,\n",
      "\\end{equation}\n",
      "the next is the update step;\n",
      "\\begin{equation}\n",
      "   \\gausmean_k = \\frac{\\sum_n \\indic{\\olabind_n = k} \\obsind_n}\n",
      "      {\\sum_n \\indic{\\olabind_n = k}}.\n",
      "\\end{equation}\n",
      "These two steps are iterated until the square loss in objective has \n",
      "converged, and that's it! This is sometimes also referred to as the [Expectation-Maximisation (EM) algorithm](http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) because of its relationship to Gausian mixture models - more on this soon.\n",
      "\n",
      "Unfortunately this is not guaranteed to converge to a global minimum of the objective function, and usually many random initialisations (random choices of $\\obsind_n$ for the initial $\\gausmean_k$) have to be attempted to find the best solution. This algorithm is very fast in practice though. Another disadvantage is that the number of clusters, $K$, has to be specified in advance. Perhaps more of a concern is that clusters are assumed to be essentially spherical in shape because of the Euclidean distance used, which is quite often an over-simplification. It is also useful to have probabilistic assignments, $\\probC{\\olabind_n = k}{\\obsind_n}$ rather than hard assignments. Gaussian mixture models solve these last two problems."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercises\n",
      "\n",
      "1) Have a go at calculating the hard assignments, $\\olabind_n$, for the data, $\\obsind_n$, and means/centres, $\\gausmean_k$, below. Also try to plot the results using the provided function.\n",
      ">In some cases it may make things much easier to represent $\\olaball$ as an NxK boolean array that only has one 1 or True value in each row."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import tututils as tut\n",
      "\n",
      "# Load X from a dataset generation function from tututils.py\n",
      "# Load M from a \" \" \" \"\n",
      "# Now get the students to calculate Z from X and M, probably using numpy's \n",
      "#  argmin()\n",
      "# TODO provide a plotting function "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2) Now let's update the means, $\\gausmean_k$ given the assignments, $\\olabind_n$, you calculated in the previous exercise, and replot the results -- note what happens to the plot (TODO)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3) Let's tie it all together now, and implement the K-means clustering algorithm in its entirety, you can use the following code as a template, or you can make your own from scratch. Once the algorithm works and converges, plot the final result to see how it differs from the above plots. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make a funcion template for them to follow\n",
      "def kmeans(X, M_init, maxit=100):\n",
      "    \n",
      "    # initialise\n",
      "    \n",
      "    # while not converged and i < maxit\n",
      "    #   E-step, update indicators \n",
      "    #   M-step, update means\n",
      "    #   Calculate loss function\n",
      "    \n",
      "    return Z, M"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try a different initialisation for K-means and study the result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Nake new initialisation\n",
      "# Call your kmeans function\n",
      "# plot the final result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Gaussian Mixture Models\n",
      "[Gaussian mixture models](http://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model) (GMMs) can be viewed as a probablistic generalisation of K-means (i.e. probabilistic cluster assignments) with the added ability to learn ellipsoidal clusters. In a GMM each observation is distributed according to a weighted sum of $K$ multivariate Gaussian distributions;\n",
      "\\begin{equation}\n",
      "   \\obsind_n \\sim \\sum^K_{k=1} \\wgtmix_k \\gausC{\\obsind_n}{\\gausmean_k,\n",
      "      \\gauscov_k}.\n",
      "\\end{equation}\n",
      "Here $\\wgtind = [\\wgtmix_1,\\ldots,\\wgtmix_k,\\ldots,\\wgtmix_K]\\transpose$ and $\\wgtmix_k \\in [0,1]$, with $\\sum_k \\wgtmix_k = 1$. $\\gauscov_k$ is a covariance matrix that describes the \"spread\" of uncertainty in the Gaussian distribtion, which is usually ellipsoidal in shape. Here is an example of what a GMM may look like in two dimensions (we have plotted the 65% probability mass contours for the Gaussians):\n",
      "\n",
      "![2D GMM](clusters.png)\n",
      "\n",
      "The weights of each Gaussian have in this mixture have not been explicitly represented, but they are implicit in the amount of observations (darkness of the voxels) they are modelling.\n",
      "\n",
      ">The following is a brief explanation of a GMM, it is more complex than K-means, so do not worry if you don't understand it fully - a full grasp of GMMs is not required for the exercises.\n",
      "\n",
      "We need a way to explicitly assign observations to mixtures or clusters. The same latent variable, $\\olabind_n$, used in K-means is introduced here as an auxiliary variable for this purpose, by inducing the following conditional relationship;\n",
      "\\begin{equation}\n",
      "   \\probC{\\obsind_n}{\\olabind_n} =  \\prod^K_{k=1}\n",
      "      \\gausC{\\obsind_n}{\\gausmean_k, \\gauscov_k}^\\indic{\\olabind_n=k},\n",
      "\\end{equation}\n",
      "so given a cluster, $\\probC{\\obsind_n}{\\olabind_k=k} = \\gausC{\\obsind_n}\n",
      "{\\gausmean_k, \\gauscov_k}$. Now it can be seen that each cluster is\n",
      "modelled as a single Gaussian with a full covariance matrix. This auxiliary\n",
      "variable is itself distributed according to a Categorical distribution (same as a Multinomial distribution but with only one observation);\n",
      "\\begin{equation}\n",
      "   \\olabind_n \\sim \\categ{\\wgtind} \n",
      "   = \\prod^K_{k=1} \\wgtmix_k^\\indic{\\olabind_n = k}.\n",
      "\\end{equation}\n",
      "The joint distribution for this GMM can be written as,\n",
      "\\begin{equation}\n",
      "   \\prob{\\obsall, \\olaball} = \\prod^N_{n=1} \\categC{\\olabind_n}{\\wgtind} \n",
      "      \\prod^K_{k=1} \n",
      "      \\gausC{\\obsind_n}{\\gausmean_k, \\gauscov_k}^\\indic{\\olabind_n=k}.\n",
      "\\end{equation}\n",
      "\n",
      "Now we need an algorithm that can learn the labels, $\\olabind_n$, cluster\n",
      "parameters, $\\gausmean_k$ and $\\gauscov_k$, and mixture weights, $\\wgtind$. Such an algorithm can be derived by maximising the *log-likelihood* of the data, \n",
      "\\begin{equation}\n",
      "    \\log \\prob{\\obsall} = \\sum_{n=1}^N\n",
      "    \\log \\sum^K_{k=1} \\wgtmix_k \\gausC{\\obsind_n}{\\gausmean_k,\n",
      "      \\gauscov_k},\n",
      "\\end{equation}\n",
      "which is acheived by setting the partial derivative of this equation with respect to each parameter and the labels in turn to zero and solving for the parameters/labels.\n",
      "\n",
      "Firstly, maximising the log-likelihood with respect to $\\olabind_n$, yields;\n",
      "\\begin{equation}\n",
      "   \\probC{\\olabind_n = k}{\\obsind_n} = \n",
      "       \\frac{\\wgtmix_k \\gausC{\\obsind_n}{\\gausmean_k, \\gauscov_k}}\n",
      "       {\\sum_l \\wgtmix_l \\gausC{\\obsind_n}{\\gausmean_l, \\gauscov_l}},\n",
      "\\end{equation}\n",
      "This is known as the *expectation* step, since the labels are probabilistically assigned their expected value given the observations and cluster parameters. \n",
      "\n",
      "Next, the parameters can be found by maximising the log-likelihood \n",
      "with respect to each parameter; \n",
      "\\begin{align}\n",
      "   \\gausmean_k &= \\frac{\\sum_n \\probC{\\olabind_n=k}{\\obsind_n} \\obsind_n}\n",
      "      {\\sum_n \\probC{\\olabind_n=k}{\\obsind_n}}, \\\\\n",
      "   \\gauscov_k &= \\frac{1}{\\sum_n \\probC{\\olabind_n=k}{\\obsind_n}}\n",
      "      \\sum^N_{n=1} \\probC{\\olabind_n = k}{\\obsind_n} (\\obsind_n -\n",
      "      \\gausmean_k)(\\obsind_n - \\gausmean_k)\\transpose, \\\\\n",
      "   \\wgtmix_k &= \\sum^N_{n=1} \\frac{\\probC{\\olabind_n = k}{\\obsind_n}}\n",
      "      {\\sum_k \\probC{\\olabind_n = k}{\\obsind_n}}.\n",
      "\\end{align}\n",
      "This is called the *maximisation* step, because the value of the\n",
      "log-likelihood is maximised with respect to the parameters given the estimated latent variables. These two steps are iterated until the log-likelihood converges. This is known as the *expectation-maximisation* EM algorithm. For all intents and purposes it is the same algorithm used to learn K-means. \n",
      "\n",
      "Unfortunately this algorithm has a few drawbacks. Like K-means, it is only\n",
      "guaranteed to converge to a local maximum of the likelihood function. Also, the Gaussian cluster updates require a full $D \\times D$ covariance matrix inversion, which has an $\\bigO{D^3}$ computational cost. This can be circumvented by using diagonal covariance Gaussian clusters, or other distributions such as Multinomial, that have only $\\bigO{D}$ computational cost. Though some expressive power is lost since inter-dimensional correlation is not modelled.\n",
      "\n",
      "Another drawback is that this algorithm still cannot choose $K$. One way to\n",
      "allow the EM algorithm to choose $K$ is to include a penalty, or\n",
      "regulariser, for having too many parameters. In this way the maximum-likelihood fitting objective can be traded off against a model complexity penalty. Some popular penalties are the [Akaike information criterion](http://en.wikipedia.org/wiki/Akaike_information_criterion) and the [Bayesian information criterion](http://en.wikipedia.org/wiki/Bayesian_information_criterion). These criterion tend to under-penalise model complexity, and are sometimes computationally costly to calculate. Another way to choose $K$ is to use a fully Bayesian treatment, where we place prior distributions on the parameters (e.g. mean, covariance and weights), and then optimise the [log *marginal* likelihood](http://en.wikipedia.org/wiki/Marginal_likelihood) of the model, which also naturally incorporates a penalty for complexity. In the case of mixture models, the Bayesian learning algorithms can have very little additional computational cost compared to EM. For more information on EM algorithms for mixture models, see (Bishop, 2006) Chapters 9 and 10.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercises\n",
      "\n",
      "1) "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Topic Models\n",
      "The purpose of topic modelling is to generally learn some low-dimensional representation of a large collection of textual documents, called a *corpus*. The representation can be used to summarize a corpus, or for retreival of documents that are similar to a query document.\n",
      "\n",
      "Typically topic models are learned by analysing the distribution of words within a corpus, in particular how they cluster together within documents that have similar subjects. These clusters of words are referred to as \"topics\", and documents may comprise several topics in different amounts. These topics are essentially the low-dimensional representation of documents learned by topic models. \n",
      "\n",
      "## Latent Dirichlet Allocation\n",
      "One of the most well known topic models is [latent Dirichlet allocation](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) (Blei, 2003), also known as multinomial PCA (Buntine, 2002). \n",
      "\n",
      "Typically a corpus is broken down as follows:\n",
      "* There are $J$ documents in a corpus,\n",
      "* there are $N_j$ words in each document, $j$,\n",
      "* a word, $\\sobsind_{nj}$, is represented as an *index* into a vocabulary, i.e. $\\sobsind_{jn} \\in \\cbrac{1,\\ldots,D}$ where $D$ is the size of the vocabulary.\n",
      "\n",
      "So we can think of our whole dataset (corpus) being made up of sub-datasets (documents), $\\obsall = \\cbrac{\\obsall_j}^J_{j=1}$, which are collections of words in each document, $\\obsall_j = \\cbrac{\\sobsind_{jn}}^{N_j}_{n=1}$. This is called a *bag of words* model, because order of the words is assumed unimportant. This simplifying assumption is known as the exchangeability assumption. \n",
      "\n",
      "LDA models words in a document as drawn from a *per-document* mixture\n",
      "of Categorical distributions,\n",
      "\\begin{equation}\n",
      "   \\sobsind_{jn} \\sim \\sum^K_{k=1} \\wgtmix_{jk} \n",
      "      \\categC{\\sobsind_{jn}}{\\mwgtind_k},\n",
      "\\end{equation}\n",
      "where there are $K$ clusters or topics, and $\\mwgtind_k$ are the parameters of the categorical distribution (which are a vector of weights that sum to one).\n",
      "\n",
      "This is very similar to the GMM we saw before, the differences being that we now have Categorical clusters as opposed to Gaussian clusters, and that we have mixture weights, $\\wgtmix_{jk}$, *specific to each document*. What is nice is that the clusters/topics are *shared* over all documents. This enables us to view these topic/cluster weights as a low-dimensional description of the original document, i.e. a mixture of topics!\n",
      "\n",
      "This is a Bayesian model, and has a Dirichlet prior placed on the document weights $\\wgtind_j \\sim\n",
      "\\dir{\\dirall}$ (where $\\wgtind_j = \\cbrac{\\wgtmix_{j1}, \\ldots, \\wgtmix_{jK}}$), and sometimes a prior is also placed on $\\mwgtind_k \\sim\n",
      "\\dir{\\dircall}$, which is called smoothed LDA.\n",
      "\n",
      "LDA can use fast sampling techniques, such as Gibbs sampling, or approximate marginal likelihood techniques, such as variational Bayes, for\n",
      "learning the model latent variables and hyper-parameters. Limitations have also been found with LDA. For example, it is not effective in choosing the\n",
      "number of topics ($K$), and the symmetric Dirichlet prior over topic weights, has been found to be too restrictive. Hierarchical Dirichlet processes (Teh, 2006) have been developed to overcome both of these issues (but is beyond the scope of this tutorial)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercises\n",
      "\n",
      "1) "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bibliography\n",
      "\n",
      "* (Bishop, 2006) C. M. Bishop. Pattern Recognition and Machine Learning. Springer Science+Business Media, Cambridge, UK, 2006.\n",
      "\n",
      "* (Blei, 2003) D. M. Blei and M. I. Jordan. Modeling annotated data. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR \u201903, pages 127\u2013134, New York, NY, USA, 2003. ACM. ISBN 1-58113-646-3.\n",
      "\n",
      "* (Buntine, 2002) W. Buntine. Variational extensions to EM and multinomial PCA. Machine Learning: ECML 2002, pages 23\u201334, 2002.\n",
      "\n",
      "* (Teh, 2006) Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566\u20131581, 2006.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}