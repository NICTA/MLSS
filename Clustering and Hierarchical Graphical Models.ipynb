{
 "metadata": {
  "name": "",
  "signature": "sha256:016177156906eb520f528e25c658f4171870469a9770e9f295c8f89580680ad5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\newcommand{\\brac}   [1] {{ \\left( #1 \\right) }}\n",
      "\\newcommand{\\sbrac}  [1] {{ \\left[ #1 \\right] }}\n",
      "\\newcommand{\\cbrac}  [1] {{ \\left\\{ #1 \\right\\} }}\n",
      "\\newcommand{\\abrac}  [1] {{ \\langle #1 \\rangle }}\n",
      "\\newcommand{\\real}   [1] {{\\mathbb{R}^{#1}}} \n",
      "\\newcommand{\\lnorm}  [1] {{\\ell_{#1}}}    \n",
      "\\newcommand{\\norm}   [2] {{\\|{#1}\\|_{#2}}} \n",
      "\\newcommand{\\abs}    [1] {{\\lvert{#1}\\rvert}}     \n",
      "\\newcommand{\\nullsp} [1] {{\\mathrm{Null}\\!\\brac{#1}}}\n",
      "\\newcommand{\\bigo}   [1] {{\\mathcal{O}\\!\\brac{#1}}} \n",
      "\\newcommand{\\trace}  [1] {{\\mathrm{Tr}\\!\\brac{#1}}}\n",
      "\\newcommand{\\argmax} [1] {{\\underset{#1}{\\arg\\max~}}}\n",
      "\\newcommand{\\argmin} [1] {{\\underset{#1}{\\arg\\min~}}}\n",
      "\\newcommand{\\indic}  [1] {{{\\mathbf{1}\\!\\sbrac{#1}}}}\n",
      "\\newcommand{\\digam}  [1] {{\\Psi\\!\\brac{#1}}}\n",
      "\\newcommand{\\gamfn}  [1] {{\\Gamma\\!\\brac{#1}}}\n",
      "\\newcommand{\\gamfnD} [1] {{\\Gamma_D\\!\\brac{#1}}}\n",
      "\\newcommand{\\transpose}  {{^{\\top}\\!}}\t\t\n",
      "\\newcommand{\\deter}  [1] {{\\left|{#1}\\right|}}\n",
      "\\newcommand{\\prob}   [1] {{p\\!\\brac{#1}}}  \n",
      "\\newcommand{\\probC}  [2] {{p\\!\\brac{#1|#2}}}\n",
      "\\newcommand{\\qrob}   [1] {{q\\!\\brac{#1}}} \n",
      "\\newcommand{\\qrobC}  [2] {{q\\!\\brac{#1|#2}}} \n",
      "\\newcommand{\\gaus}   [1] {{\\mathcal{N}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\gausC}  [2] {{\\mathcal{N}\\!\\brac{#1|#2}}}\n",
      "\\newcommand{\\gam}    [1] {{\\mathrm{Gamma}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\gamC}   [2] {{\\mathrm{Gamma}\\!\\brac{#1|#2}}}\n",
      "\\newcommand{\\betad}  [1] {{\\mathrm{Beta}\\!\\brac{#1}}}\n",
      "\\newcommand{\\betaC}  [2] {{\\mathrm{Beta}({#1}|{#2})}}\n",
      "\\newcommand{\\dir}    [1] {{\\mathrm{Dir}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\dirC}   [2] {{\\mathrm{Dir}\\!\\brac{#1|#2}}}\n",
      "\\newcommand{\\gdir}   [1] {{\\mathrm{GDir}\\!\\brac{#1}}}  \n",
      "\\newcommand{\\gdirC}  [2] {{\\mathrm{GDir}\\!\\brac{#1|#2}}}\n",
      "\\newcommand{\\DP}     [1] {{\\mathrm{DP}\\!\\brac{#1}}}\n",
      "\\newcommand{\\SB}     [1] {{\\mathrm{SB}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\wish}   [1] {{\\mathcal{W}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\wishC}  [2] {{\\mathcal{W}\\!\\brac{#1|#2}}}\t\n",
      "\\newcommand{\\categ}  [1] {{\\mathrm{Cat}\\!\\brac{#1}}}\n",
      "\\newcommand{\\categC} [2] {{\\mathrm{Cat}\\!\\brac{#1|#2}}}\n",
      "\\newcommand{\\ncons}  [1] {{\\mathcal{Z}_{#1}}}\n",
      "\\newcommand{\\bigO}   [1] {{\\mathcal{O}!\\brac{#1}}}\n",
      "\\newcommand{\\obsall}{\\mathbf{X}}\n",
      "\\newcommand{\\obsind}{\\mathbf{x}}\n",
      "\\newcommand{\\sobsind}{x}\n",
      "\\newcommand{\\iobsall}{\\mathbf{W}}\n",
      "\\newcommand{\\iobsind}{\\mathbf{w}}\n",
      "\\newcommand{\\obsbar}{\\bar{\\mathbf{x}}}\n",
      "\\newcommand{\\obscov}{\\bar{\\mathbf{S}}}\n",
      "\\newcommand{\\olaball}{\\mathbf{Z}}\n",
      "\\newcommand{\\olabgrp}{\\mathbf{z}}\n",
      "\\newcommand{\\olabind}{z}\n",
      "\\newcommand{\\ilaball}{\\mathbf{Y}}\n",
      "\\newcommand{\\ilabind}{y}\n",
      "\\newcommand{\\allparam}{\\boldsymbol\\Theta}\n",
      "\\newcommand{\\allhyper}{\\boldsymbol\\Xi}\n",
      "\\newcommand{\\mwgtall}{\\mathbf{B}}\n",
      "\\newcommand{\\mwgtind}{\\boldsymbol{\\beta}}\n",
      "\\newcommand{\\mwgtmix}{\\beta}\n",
      "\\newcommand{\\wgtall}{\\boldsymbol\\Pi}\n",
      "\\newcommand{\\wgtind}{\\boldsymbol\\pi}\n",
      "\\newcommand{\\wgtmix}{\\pi}\n",
      "\\newcommand{\\stkall}{\\mathbf{V}}\n",
      "\\newcommand{\\stkmix}{v}\n",
      "\\newcommand{\\expall}{\\boldsymbol\\Theta}\n",
      "\\newcommand{\\expmix}{\\theta}\n",
      "\\newcommand{\\hypexn}{\\eta}\n",
      "\\newcommand{\\hypexv}{\\boldsymbol\\nu}\n",
      "\\newcommand{\\gausmean}{\\boldsymbol\\mu}\n",
      "\\newcommand{\\gauscov}{\\boldsymbol\\Lambda}\n",
      "\\newcommand{\\hypgausm}{\\mathbf{m}}\n",
      "\\newcommand{\\hypgausb}{\\gamma}\n",
      "\\newcommand{\\hypgausW}{\\boldsymbol{\\Omega}}\n",
      "\\newcommand{\\hypgausp}{\\rho}\n",
      "\\newcommand{\\igausmean}{\\boldsymbol\\eta}\n",
      "\\newcommand{\\igauscov}{\\boldsymbol\\Psi}\n",
      "\\newcommand{\\ihypgausm}{\\mathbf{h}}\n",
      "\\newcommand{\\ihypgausb}{\\delta}\n",
      "\\newcommand{\\ihypgausW}{\\boldsymbol{\\Phi}}\n",
      "\\newcommand{\\ihypgausp}{\\xi}\n",
      "\\newcommand{\\dirall}{\\alpha}\n",
      "\\newcommand{\\dirmix}{\\alpha}\n",
      "\\newcommand{\\dircall}{\\theta}\n",
      "\\newcommand{\\dircmix}{\\theta}\n",
      "\\newcommand{\\gdaall}{a}\n",
      "\\newcommand{\\gdball}{b}\n",
      "\\newcommand{\\gdamix}{a}\n",
      "\\newcommand{\\gdbmix}{b}\n",
      "\\newcommand{\\Cwidthi} {\\ensuremath{C_{w,i}}}\n",
      "\\newcommand{\\Cwidths} {\\ensuremath{C_{w,s}}}\n",
      "\\newcommand{\\ICAdic}  {\\ensuremath{\\mathbf{D}}}\n",
      "\\newcommand{\\ICAdicp} {\\ensuremath{\\mathbf{D}^+}}\n",
      "\\newcommand{\\ICAresp} {\\ensuremath{\\mathbf{r}}}\n",
      "\\newcommand{\\Segment} {\\ensuremath{S_{jin}}}\n",
      "\\newcommand{\\dimim}   {\\ensuremath{D_\\mathrm{im}}}\n",
      "\\newcommand{\\dimseg}  {\\ensuremath{D_\\mathrm{seg}}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Clustering\n",
      "Clustering is one of the oldest data exploration methods. The objective is for an algorithm to discover sets of *similar* points, or observations, within a larger dataset. These sets are called *clusters*.  Similarity is almost always characterised by some distance function between observations, such as Euclidean $\\ell_2$. Some of the more simple algorithms require the number of clusters to be specified in advance, while others can also infer this from the data, usually given other assumptions. K-means was one of the first and still most popular algorithms designed for this task."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##K-means\n",
      "\n",
      "The objective of K-means clustering is to find $K$ clusters of observations\n",
      "within a dataset of $N$ observations, $\\obsall = \\cbrac{\\obsind_n}^N_{n=1}$, where $\\obsind_n \\in\n",
      "\\real{D}$. These clusters are characterised by their means, $\\mathbf{M} =\n",
      "\\cbrac{\\gausmean_k}^K_{k=1}$ where $\\gausmean_k \\in \\real{D}$. Each observation\n",
      "is assigned to a cluster mean using a label $\\olabind_n \\in \\cbrac{1,\\ldots,K}$,\n",
      "and $\\olaball = \\cbrac{\\olabind_n}^N_{n=1}$. The objective of K-means is to\n",
      "minimise the square loss, or reconstruction error,\n",
      "\\begin{equation}\n",
      "   \\min_{\\mathbf{M},\\olaball} \\sum^N_{n=1} \\sum^K_{k=1} \\indic{\\olabind_n = k}\n",
      "      \\norm{\\obsind_n - \\gausmean_k}{2}^2.\n",
      "\\end{equation}\n",
      "Here $\\indic{\\cdot}$ is an indicator function that evaluates to 1 when the\n",
      "condition in the brackets is true, and 0 otherwise. $\\norm{\\cdot}{2}$ is an\n",
      "$\\ell_2$ norm, or Euclidean distance. This is solved with two simple alternating\n",
      "steps. The first is the assignment step;\n",
      "\\begin{equation}\n",
      "   \\olabind_n = \\argmin{k} \\norm{\\obsind_n - \\gausmean_k}{2}^2,\n",
      "\\end{equation}\n",
      "the next is the update step;\n",
      "\\begin{equation}\n",
      "   \\gausmean_k = \\frac{\\sum_n \\indic{\\olabind_n = k} \\obsind_n}\n",
      "      {\\sum_n \\indic{\\olabind_n = k}}.\n",
      "\\end{equation}\n",
      "These two steps are iterated until the square loss in objective has \n",
      "converged, and that's it! This is sometimes also referred to as the Expectation-Maximisation (EM) algorithm because of its relationship to Gausian mixture models -- more on this soon.\n",
      "\n",
      "Unfortunately this is not guaranteed to converge to a global minimum, and\n",
      "usually many random initialisations (random choices of $\\obsind_n$ for the\n",
      "initial $\\gausmean_k$) have to be attempted to find the best solution. This\n",
      "algorithm is very fast in practice though. Another disadvantage is that the\n",
      "number of clusters, $K$, has to be specified in advance. Perhaps more of a\n",
      "concern is that clusters are assumed to be essentially spherical because of the\n",
      "Euclidean distance used, which is quite often an over-simplification. It is also\n",
      "useful to have probabilistic assignments, $\\probC{\\olabind_n = k}{\\obsind_n}$\n",
      "rather than hard assignments. Gaussian mixture models solve these last two\n",
      "problems."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercises\n",
      "\n",
      "1) Have a go at calculating the hard assignments, $\\olabind_n$, for the data, $\\obsind_n$, and means/centres, $\\gausmean_k$, below. Also try to plot the results using the provided function.\n",
      ">It makes things much easier to represent $\\olaball$ as an NxK boolean array that only has one 1 or True value in each row."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import tututils as tut\n",
      "\n",
      "# Load X from a dataset generation function from tututils.py\n",
      "# Load M from a \" \" \" \"\n",
      "# Now get the students to calculate Z from X and M, probably using numpy's \n",
      "#  argmin()\n",
      "# TODO provide a plotting function "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2) Now let's update the means, $\\gausmean_k$ given the assignments, $\\olabind_n$, you calculated in the previous exercise, and replot the results -- note what happens to the plot (TODO)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3) Let's tie it all together now, and implement the K-means clustering algorithm in its entirety, you can use the following code as a template, or you can make your own from scratch. Once the algorithm works and converges, plot the final result to see how it differs from the above plots. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make a funcion template for them to follow\n",
      "def kmeans(X, M_init, maxit=100):\n",
      "    \n",
      "    # initialise\n",
      "    \n",
      "    # while not converged and i < maxit\n",
      "    #   E-step, update indicators \n",
      "    #   M-step, update means\n",
      "    #   Calculate loss function\n",
      "    \n",
      "    return Z, M"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try a different initialisation for K-means and study the result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Nake new initialisation\n",
      "# Call your kmeans function\n",
      "# plot the final result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Gaussian Mixture Models\n",
      "In a \\ac{GMM}, see \\citet{Bishop2006}, each observation is distributed according\n",
      "to a weighted sum of Gaussian distributions;\n",
      "\\begin{equation}\n",
      "   \\obsind_n \\sim \\sum^K_{k=1} \\wgtmix_k \\gausC{\\obsind_n}{\\gausmean_k,\n",
      "      \\gauscov_k^{-1}}.\n",
      "\\end{equation}\n",
      "Here $\\wgtind = [\\wgtmix_1,\\ldots,\\wgtmix_k,\\ldots,\\wgtmix_K]\\transpose$ and\n",
      "$\\wgtmix_k \\in [0,1]$, with $\\sum_k \\wgtmix_k = 1$. Also, Gaussian precision is\n",
      "used here instead of covariance ($\\gauscov^{-1}_k = \\boldsymbol\\Sigma_k$) for\n",
      "consistency later. What is still missing is a way to explicitly assign\n",
      "observations to mixtures or clusters. The same latent variable, $\\olabind_n$,\n",
      "used in K-means is introduced here as an auxiliary variable for this purpose, by\n",
      "inducing the following conditional relationship;\n",
      "\\begin{equation}\n",
      "   \\probC{\\obsind_n}{\\olabind_n} =  \\prod^K_{k=1}\n",
      "      \\gausC{\\obsind_n}{\\gausmean_k, \\gauscov_k^{-1}}^\\indic{\\olabind_n=k},\n",
      "\\end{equation}\n",
      "so given a cluster, $\\probC{\\obsind_n}{\\olabind_k=k} = \\gausC{\\obsind_n}\n",
      "{\\gausmean_k, \\gauscov_k^{-1}}$. Now it can be seen that each cluster is\n",
      "modelled as a single Gaussian, with a full covariance matrix. This auxiliary\n",
      "variable is itself distributed according to a Categorical distribution;\n",
      "\\begin{equation}\n",
      "   \\olabind_n \\sim \\categ{\\wgtind} \n",
      "   = \\prod^K_{k=1} \\wgtmix_k^\\indic{\\olabind_n = k}.\n",
      "\\end{equation}\n",
      "The joint, or ``complete-data'' likelihood is (omitting the conditional\n",
      "parameters),\n",
      "\\begin{equation}\n",
      "   \\prob{\\obsall, \\olaball} = \\prod^N_{n=1} \\categC{\\olabind_n}{\\wgtind} \n",
      "      \\prod^K_{k=1} \n",
      "      \\gausC{\\obsind_n}{\\gausmean_k, \\gauscov_k^{-1}}^\\indic{\\olabind_n=k}.\n",
      "\\end{equation}\n",
      "The graphical model of this joint is in \\autoref{sfig:gmm_joint}. For a single\n",
      "observation, if the auxiliary variable is marginalised (summed) out of\n",
      "\\autoref{eq:gmm_joint}, we are left with the marginal density in\n",
      "\\autoref{eq:bac_gmm}.\n",
      "\n",
      "\\begin{figure}[tb]\n",
      "   \\centering\n",
      "   \\subfloat[][GMM]{\\input{ChapterBackground/Figures/gmm.tex}\n",
      "      \\label{sfig:gmm_joint}} \\hspace{2cm}\n",
      "   \\subfloat[][BGMM]{\\input{ChapterBackground/Figures/bgmm.tex}\n",
      "      \\label{sfig:bgmm_joint}}\n",
      "   \\caption[Graphical models of a \\ac{GMM} and \\ac{BGMM}]{%\n",
      "      Graphical models of a \\ac{GMM} and \\ac{BGMM}. Circles (nodes) are\n",
      "      distributions, points are point estimates of parameters, and arrows (arcs)\n",
      "      are conditional relationships. The shaded circle is observable, and plates\n",
      "      denote replication over the respective index.}\n",
      "   \\label{fig:gmm_mod}\n",
      "\\end{figure}\n",
      "\n",
      "Now we need an algorithm that can learn the labels, $\\olabind_n$, cluster\n",
      "parameters, $\\gausmean_k$ and $\\gauscov_k$, and mixture weights, $\\wgtind$. Such\n",
      "an algorithm can be derived by maximising (taking partial derivatives and\n",
      "setting to zero) the log-likelihood of the data, $\\log \\prob{\\obsall} = \\sum_n\n",
      "\\log \\prob{\\obsind_n}$ from \\autoref{eq:bac_gmm}, conditioned on the model\n",
      "parameters and latent variables. Firstly, maximising the log-likelihood with\n",
      "respect to $\\olabind_n$, yields;\n",
      "\\begin{equation}\n",
      "   \\probC{\\olabind_n = k}{\\obsind_n} = \\frac{1}{\\ncons{\\olabind_n}} \\wgtmix_k\n",
      "      \\gausC{\\obsind_n}{\\gausmean_k, \\gauscov_k^{-1}},\n",
      "\\end{equation}\n",
      "where $\\ncons{\\olabind_n} = \\sum_k \\wgtmix_k \\gausC{\\obsind_n}{\\gausmean_k, \n",
      "   \\gauscov_k^{-1}}$. This is known as the \\emph{expectation} step, since the\n",
      "labels are assigned their expected value given the observations and cluster \n",
      "parameters. Next, the parameters can be found by maximising the log-likelihood \n",
      "with respect to each parameter; \n",
      "\\begin{align}\n",
      "   \\gausmean_k &= \\frac{\\sum_n \\probC{\\olabind_n=k}{\\obsind_n} \\obsind_n}\n",
      "      {\\sum_n \\probC{\\olabind_n=k}{\\obsind_n}}, \\\\\n",
      "   \\gauscov_k^{-1} &= \\frac{1}{\\sum_n \\probC{\\olabind_n=k}{\\obsind_n}}\n",
      "      \\sum^N_{n=1} \\probC{\\olabind_n = k}{\\obsind_n} (\\obsind_n -\n",
      "      \\gausmean_k)(\\obsind_n - \\gausmean_k)\\transpose, \\\\\n",
      "   \\wgtmix_k &= \\sum^N_{n=1} \\frac{\\probC{\\olabind_n = k}{\\obsind_n}}\n",
      "      {\\sum_k \\probC{\\olabind_n = k}{\\obsind_n}}.\n",
      "\\end{align}\n",
      "\n",
      "This is called the \\emph{maximisation} step, because the value of the\n",
      "log-likelihood is maximised with respect to the parameters given the estimated\n",
      "latent variables. These two steps are iterated until the log-likelihood\n",
      "converges. This is known as the \\ac{EM} algorithm, and as we can see, for all\n",
      "intents and purposes it is the same algorithm used to learn K-means. The\n",
      "exceptions being that a probabilistic assignment is learned, $\\probC{\\olabind_n\n",
      "   = k}{\\obsind_n}$, and \\emph{Mahalanobis}\\footnote{$\\mathrm{dist_{Mahal.}} =\n",
      "   (\\obsind_n - \\gausmean_k)\\transpose \\gauscov_k (\\obsind_n - \\gausmean_k)$}\n",
      "distances are used (from the Gaussian clusters) as opposed to Euclidean\n",
      "distance. This allows clusters to have arbitrary ellipsoidal shapes.\n",
      "Furthermore, Gaussian clusters do not have to be used, for instance Multinomial\n",
      "clusters are another popular choice \\cite{Masada2007,Blei2003,Bouguila2008a}.\n",
      "\n",
      "Unfortunately this algorithm has a few drawbacks. Like K-means, it is only\n",
      "guaranteed to converge to a local maximum of the likelihood function. Also, the\n",
      "Gaussian cluster updates require a full $D \\times D$ covariance matrix\n",
      "inversion, which has a $\\bigO{D^3}$ computational cost. This can be circumvented\n",
      "by using diagonal covariance Gaussian clusters, or other distributions such as\n",
      "Multinomial, that have only $\\bigO{D}$ computational cost. Though some\n",
      "expressive power is lost since inter-dimensional correlation is not modelled.\n",
      "\n",
      "Another drawback is that this algorithm still cannot choose $K$. One way to\n",
      "allow the \\ac{EM} algorithm to choose $K$ is to include a penalty, or\n",
      "regulariser, for having too many parameters. In this way the maximum-likelihood\n",
      "fitting objective can be traded off against a model complexity penalty. Some\n",
      "popular penalties are the \\ac{AIC} \\cite{Akaike1974} and the\n",
      "\\ac{BIC}~\\cite{Schwarz1978}. These criterion tend to under-penalise model\n",
      "complexity \\cite{Beal2003}, and are sometimes computationally costly to\n",
      "calculate. Another way to choose $K$ is to use a fully Bayesian treatment, which\n",
      "in fact ``averages'' over all models to find the most simple model with the best\n",
      "fit for the data.  In the case of mixture models, the learning algorithms can\n",
      "have very little additional computational cost compared to \\ac{EM}. For more\n",
      "details on maximum-likelihood \\acp{GMM}, see \\cite[Ch. 9]{Bishop2006}.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Topic Models\n",
      "In this section a very brief overview of topic models is presented, with the\n",
      "intent of giving the reader a ``flavour'' of the field. Most of these models are\n",
      "not directly used in this thesis, however some of the models derived in later\n",
      "chapters have been influenced by these models.\n",
      "\n",
      "The purpose of topic modelling is to generally perform inference on a large\n",
      "collection of textual documents, called a \\emph{corpus} \\cite{Blei2003,Teh2006}.\n",
      "Inference may be to retrieve documents that are similar to various search terms\n",
      "(such as web search), or to discover collections of like documents from their\n",
      "distributions of words -- essentially document clustering or classification.\n",
      "\n",
      "##Latent Dirichlet Allocation\n",
      "To rectify these problems with \\ac{pLSA}, \\citet{Blei2003} created \\ac{LDA}. It\n",
      "begins by defining a word, $\\sobsind_{jn} \\in \\cbrac{1,\\ldots,D}$, as an index\n",
      "into a vocabulary of $D$ word types. There are $N_j$ words in a document,\n",
      "$\\obsall_j = \\cbrac{\\sobsind_{jn}}^{N_j}_{n=1}$, with $J$ documents in a corpus,\n",
      "$\\obsall = \\cbrac{\\obsall_j}^J_{j=1}$. This is called a \\ac{BOW} model, because\n",
      "order of the words is assumed unimportant. This simplifying assumption is known\n",
      "as the exchangeability assumption. \n",
      "\n",
      "\\ac{LDA} models words in a document as drawn from a \\emph{per-document} mixture\n",
      "of Categorical distributions,\n",
      "\\begin{equation}\n",
      "   \\sobsind_{jn} \\sim \\sum^K_{k=1} \\wgtmix_{jk} \n",
      "      \\categC{\\sobsind_{jn}}{\\mwgtind_k},\n",
      "\\end{equation}\n",
      "where $\\mwgtind_k$ is also a vector of weights. These $K$ categorical clusters\n",
      "are called 'topics', and are \\emph{shared} between documents. What is nice\n",
      "about this model is that each document can now be described as a mixture of\n",
      "these $K$ topics, $\\wgtind_j$. Typically $K \\ll D$, and so this model is\n",
      "equivalent to discrete \\ac{PCA} \\cite{Buntine2002}, for dimensionality\n",
      "reduction. \\ac{LDA} can generalise to unseen documents, and the mixture weights\n",
      "can also be used in a similar fashion to the latent ``semantic space'' variable\n",
      "in \\ac{LSA}, while having a real semantic meaning.\n",
      "\n",
      "This is a Bayesian model, and has a prior placed on each $\\wgtind_j \\sim\n",
      "\\dir{\\dirall}$, and sometimes a prior is also placed on $\\mwgtind_k \\sim\n",
      "\\dir{\\dircall}$, which is called smoothed \\ac{LDA}. The graphical model of\n",
      "smoothed \\ac{LDA} is presented in \\autoref{fig:top_mods}. \n",
      "\n",
      "\\ac{LDA} can use \\ac{VB} or sampling techniques, such as Gibbs sampling, for\n",
      "learning the model latent variables and hyper-parameters. Limitations have also\n",
      "been found with \\ac{LDA}. For example, it is not effective in choosing the\n",
      "number of topics ($K$), and the symmetric Dirichlet prior over topic weights,\n",
      "$\\dirC{\\wgtind_j}{\\dirall}$, has been found to be too restrictive~\\cite{Teh2006,\n",
      "   Wallach2009}.\n",
      "\n",
      "\\begin{figure}[tb]\n",
      "   \\centering\n",
      "   \\input{ChapterBackground/Figures/lda.tex}\n",
      "   \\caption[Graphical model of smoothed \\ac{LDA}]{%\n",
      "      Graphical model of smoothed \\ac{LDA}. This is very similar to a regular\n",
      "      Bayesian mixture model, but replicated over $J$ documents, with cluster, \n",
      "      or topic, sharing between documents.}\n",
      "   \\label{fig:top_mods}\n",
      "\\end{figure}\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}