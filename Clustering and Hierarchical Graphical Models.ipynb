{
 "metadata": {
  "name": "",
  "signature": "sha256:edcee36c280a092e6a33beac3d082b14fa232eb829964e4a365c992133b26559"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Clustering and Hierarchical Graphical Models, MLSS 2015\n",
      "\n",
      "**Authors**: [Daniel Steinberg](http://www.daniel-steinberg.info/) & [Brian Thorne](http://hardbyte.bitbucket.org/)\n",
      "\n",
      "**Institute**: [NICTA](https://www.nicta.com.au/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\newcommand{\\brac}   [1] {{ \\left( #1 \\right) }}\n",
      "\\newcommand{\\sbrac}  [1] {{ \\left[ #1 \\right] }}\n",
      "\\newcommand{\\cbrac}  [1] {{ \\left\\{ #1 \\right\\} }}\n",
      "\\newcommand{\\abrac}  [1] {{ \\langle #1 \\rangle }}\n",
      "\\newcommand{\\real}   [1] {{\\mathbb{R}^{#1}}} \n",
      "\\newcommand{\\lnorm}  [1] {{\\ell_{#1}}}    \n",
      "\\newcommand{\\norm}   [2] {{\\|{#1}\\|_{#2}}} \n",
      "\\newcommand{\\abs}    [1] {{\\lvert{#1}\\rvert}}     \n",
      "\\newcommand{\\nullsp} [1] {{\\mathrm{Null}\\!\\brac{#1}}}\n",
      "\\newcommand{\\bigo}   [1] {{\\mathcal{O}\\!\\brac{#1}}} \n",
      "\\newcommand{\\trace}  [1] {{\\mathrm{Tr}\\!\\brac{#1}}}\n",
      "\\newcommand{\\argmax} [1] {{\\underset{#1}{\\arg\\max~}}}\n",
      "\\newcommand{\\argmin} [1] {{\\underset{#1}{\\arg\\min~}}}\n",
      "\\newcommand{\\indic}  [1] {{{\\mathbf{1}\\!\\sbrac{#1}}}}\n",
      "\\newcommand{\\digam}  [1] {{\\Psi\\!\\brac{#1}}}\n",
      "\\newcommand{\\gamfn}  [1] {{\\Gamma\\!\\brac{#1}}}\n",
      "\\newcommand{\\gamfnD} [1] {{\\Gamma_D\\!\\brac{#1}}}\n",
      "\\newcommand{\\transpose}  {{^{\\top}\\!}}\t\t\n",
      "\\newcommand{\\deter}  [1] {{\\left|{#1}\\right|}}\n",
      "\\newcommand{\\prob}   [1] {{p\\brac{#1}}}  \n",
      "\\newcommand{\\probC}  [2] {{p\\brac{#1|#2}}}\n",
      "\\newcommand{\\qrob}   [1] {{q\\!\\brac{#1}}} \n",
      "\\newcommand{\\qrobC}  [2] {{q\\!\\brac{#1|#2}}} \n",
      "\\newcommand{\\gaus}   [1] {{\\mathcal{N}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\gausC}  [2] {{\\mathcal{N}\\!\\brac{#1|#2}}}\n",
      "\\newcommand{\\gam}    [1] {{\\mathrm{Gamma}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\gamC}   [2] {{\\mathrm{Gamma}\\!\\brac{#1|#2}}}\n",
      "\\newcommand{\\betad}  [1] {{\\mathrm{Beta}\\!\\brac{#1}}}\n",
      "\\newcommand{\\betaC}  [2] {{\\mathrm{Beta}({#1}|{#2})}}\n",
      "\\newcommand{\\dir}    [1] {{\\mathrm{Dir}\\brac{#1}}}\t\n",
      "\\newcommand{\\dirC}   [2] {{\\mathrm{Dir}\\brac{#1|#2}}}\n",
      "\\newcommand{\\DP}     [1] {{\\mathrm{DP}\\!\\brac{#1}}}\n",
      "\\newcommand{\\SB}     [1] {{\\mathrm{SB}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\wish}   [1] {{\\mathcal{W}\\!\\brac{#1}}}\t\n",
      "\\newcommand{\\wishC}  [2] {{\\mathcal{W}\\!\\brac{#1|#2}}}\t\n",
      "\\newcommand{\\categ}  [1] {{\\mathrm{Categ}\\brac{#1}}}\n",
      "\\newcommand{\\categC} [2] {{\\mathrm{Categ}\\brac{#1|#2}}}\n",
      "\\newcommand{\\ncons}  [1] {{\\mathcal{Z}_{#1}}}\n",
      "\\newcommand{\\bigO}   [1] {{\\mathcal{O}({#1})}}\n",
      "\\newcommand{\\obsall}{\\mathbf{X}}\n",
      "\\newcommand{\\obsind}{\\mathbf{x}}\n",
      "\\newcommand{\\sobsind}{x}\n",
      "\\newcommand{\\iobsall}{\\mathbf{W}}\n",
      "\\newcommand{\\iobsind}{\\mathbf{w}}\n",
      "\\newcommand{\\obsbar}{\\bar{\\mathbf{x}}}\n",
      "\\newcommand{\\obscov}{\\bar{\\mathbf{S}}}\n",
      "\\newcommand{\\olaball}{\\mathbf{Z}}\n",
      "\\newcommand{\\olabgrp}{\\mathbf{z}}\n",
      "\\newcommand{\\olabind}{z}\n",
      "\\newcommand{\\ilaball}{\\mathbf{Y}}\n",
      "\\newcommand{\\ilabind}{y}\n",
      "\\newcommand{\\allparam}{\\boldsymbol\\Theta}\n",
      "\\newcommand{\\allhyper}{\\boldsymbol\\Xi}\n",
      "\\newcommand{\\mwgtall}{\\mathbf{B}}\n",
      "\\newcommand{\\mwgtind}{\\boldsymbol{\\beta}}\n",
      "\\newcommand{\\mwgtmix}{\\beta}\n",
      "\\newcommand{\\wgtall}{\\boldsymbol\\Pi}\n",
      "\\newcommand{\\wgtind}{\\boldsymbol\\pi}\n",
      "\\newcommand{\\wgtmix}{\\pi}\n",
      "\\newcommand{\\stkall}{\\mathbf{V}}\n",
      "\\newcommand{\\stkmix}{v}\n",
      "\\newcommand{\\expall}{\\boldsymbol\\Theta}\n",
      "\\newcommand{\\expmix}{\\theta}\n",
      "\\newcommand{\\hypexn}{\\eta}\n",
      "\\newcommand{\\hypexv}{\\boldsymbol\\nu}\n",
      "\\newcommand{\\gausmean}{\\boldsymbol\\mu}\n",
      "\\newcommand{\\gauscov}{\\boldsymbol\\Sigma}\n",
      "\\newcommand{\\hypgausm}{\\mathbf{m}}\n",
      "\\newcommand{\\hypgausb}{\\gamma}\n",
      "\\newcommand{\\hypgausW}{\\boldsymbol{\\Omega}}\n",
      "\\newcommand{\\hypgausp}{\\rho}\n",
      "\\newcommand{\\igausmean}{\\boldsymbol\\eta}\n",
      "\\newcommand{\\igauscov}{\\boldsymbol\\Psi}\n",
      "\\newcommand{\\ihypgausm}{\\mathbf{h}}\n",
      "\\newcommand{\\ihypgausb}{\\delta}\n",
      "\\newcommand{\\ihypgausW}{\\boldsymbol{\\Phi}}\n",
      "\\newcommand{\\ihypgausp}{\\xi}\n",
      "\\newcommand{\\dirall}{\\alpha}\n",
      "\\newcommand{\\dirmix}{\\alpha}\n",
      "\\newcommand{\\dircall}{\\theta}\n",
      "\\newcommand{\\dircmix}{\\theta}\n",
      "\\newcommand{\\gdaall}{a}\n",
      "\\newcommand{\\gdball}{b}\n",
      "\\newcommand{\\gdamix}{a}\n",
      "\\newcommand{\\gdbmix}{b}\n",
      "\\newcommand{\\Cwidthi} {\\ensuremath{C_{w,i}}}\n",
      "\\newcommand{\\Cwidths} {\\ensuremath{C_{w,s}}}\n",
      "\\newcommand{\\ICAdic}  {\\ensuremath{\\mathbf{D}}}\n",
      "\\newcommand{\\ICAdicp} {\\ensuremath{\\mathbf{D}^+}}\n",
      "\\newcommand{\\ICAresp} {\\ensuremath{\\mathbf{r}}}\n",
      "\\newcommand{\\Segment} {\\ensuremath{S_{jin}}}\n",
      "\\newcommand{\\dimim}   {\\ensuremath{D_\\mathrm{im}}}\n",
      "\\newcommand{\\dimseg}  {\\ensuremath{D_\\mathrm{seg}}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Clustering\n",
      "Clustering is one of the oldest data exploration methods. The objective is for an algorithm to discover sets of *similar* points, or observations, within a larger dataset. These sets are called *clusters*.  Similarity is almost always characterised by some distance function between observations, such as Euclidean $\\ell_2$. Some of the more simple algorithms require the number of clusters to be specified in advance, while others can also infer this from the data, usually given other assumptions. K-means was one of the first and still most popular algorithms designed for this task."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##K-means\n",
      "\n",
      "The objective of K-means clustering is to find $K$ clusters of observations\n",
      "within a dataset of $N$ observations, $\\obsall = \\cbrac{\\obsind_n}^N_{n=1}$, where $\\obsind_n \\in \\real{D}$. These clusters are characterised by their means, $\\mathbf{M} =\n",
      "\\cbrac{\\gausmean_k}^K_{k=1}$ where $\\gausmean_k \\in \\real{D}$. Each observation\n",
      "is assigned to a cluster mean using an integer label $\\olabind_n \\in \\cbrac{1,\\ldots,K}$,\n",
      "and $\\olaball = \\cbrac{\\olabind_n}^N_{n=1}$. The objective of K-means is to\n",
      "minimise the square loss, or reconstruction error,\n",
      "\\begin{equation}\n",
      "   \\min_{\\mathbf{M},\\olaball} \\sum^N_{n=1} \\sum^K_{k=1} \\indic{\\olabind_n = k}\n",
      "      \\norm{\\obsind_n - \\gausmean_k}{2}^2.\n",
      "\\end{equation}\n",
      "Here $\\indic{\\cdot}$ is an indicator function that evaluates to 1 when the\n",
      "condition in the brackets is true, and 0 otherwise. $\\norm{\\cdot}{2}$ is an\n",
      "$\\ell_2$ norm, or Euclidean distance. This is solved with two simple alternating\n",
      "steps. The first is the assignment step;\n",
      "\\begin{equation}\n",
      "   \\olabind_n = \\argmin{k} \\norm{\\obsind_n - \\gausmean_k}{2}^2,\n",
      "\\end{equation}\n",
      "the next is the update step;\n",
      "\\begin{equation}\n",
      "   \\gausmean_k = \\frac{\\sum_n \\indic{\\olabind_n = k} \\obsind_n}\n",
      "      {\\sum_n \\indic{\\olabind_n = k}}.\n",
      "\\end{equation}\n",
      "These two steps are iterated until the square loss in objective has \n",
      "converged, and that's it! This is sometimes also referred to as the [Expectation-Maximisation (EM) algorithm](http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) because of its relationship to Gausian mixture models - more on this soon.\n",
      "\n",
      "Unfortunately this is not guaranteed to converge to a global minimum of the objective function, and usually many random initialisations (random choices of $\\obsind_n$ for the initial $\\gausmean_k$) have to be attempted to find the best solution. This algorithm is very fast in practice though. Another disadvantage is that the number of clusters, $K$, has to be specified in advance. Perhaps more of a concern is that clusters are assumed to be essentially spherical in shape because of the Euclidean distance used, which is quite often an over-simplification. It is also useful to have probabilistic assignments, $\\probC{\\olabind_n = k}{\\obsind_n}$ rather than hard assignments. Gaussian mixture models solve these last two problems."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercises\n",
      "\n",
      "**1)** Have a go at calculating the hard assignments, $\\olabind_n$, for the data, $\\obsind_n$, from the initial means/centres, $\\gausmean_k$, provided. Also try to plot the results using the provided function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Importing some libraries and modules\n",
      "%pylab inline\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import tututils as tut"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load X from a dataset generation function from tututils\n",
      "X = tut.load_2d_simple()\n",
      "K = 3  # There are three clusters in this dataset\n",
      "N = X.shape[0]\n",
      "k_means_cluster_centers = X[np.random.randint(0, N, size=K), :]\n",
      "\n",
      "# Now calculate Z from X and k_means_cluster_centers.\n",
      "# Tip: consider using numpy's argmin()\n",
      "\n",
      "#TODO: k_means_labels = \n",
      "\n",
      "# Check your answer by plotting the clusters: \n",
      "tut.plot_2d_clusters(X, k_means_labels, k_means_cluster_centers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2)** Now have a go at calculating the cluster means, $\\gausmean_k$, given the cluster assignments, $\\olabind_n$, calculated previously. Again, plot the results using the provided function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate k_means_cluster_centers from X and k_means_labels\n",
      "\n",
      "#TODO: k_means_cluster_centers = \n",
      "\n",
      "# Check your answer by plotting the clusters:\n",
      "# TIP: You may need to re-do the labels to make the vornoi cells match the labels\n",
      "\n",
      "#TODO: re-do labels (optional)\n",
      "\n",
      "tut.plot_2d_clusters(X, k_means_labels, k_means_cluster_centers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3)** Let's tie it all together now, and implement the K-means clustering algorithm in its entirety, you can use the following code as a template, or you can make your own from scratch. Once the algorithm works and converges, plot the final result to see how it differs from the above plots. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Funcion template -- fill in the gaps\n",
      "def kmeans(X, K, maxit=100):\n",
      "    \n",
      "    # initialise\n",
      "    N, D = X.shape\n",
      "    M = X[np.random.randint(0, N, size=K), :]\n",
      "    obj = np.finfo(float).max\n",
      "    \n",
      "    # TODO: Any initialisation code\n",
      "    \n",
      "    for i in range(maxit):\n",
      "        objo = obj\n",
      "        \n",
      "        # TODO: E-step, update indicators \n",
      "        # TODO: M-step, update means\n",
      "        # TODO: Calculate loss function i.e. obj =\n",
      "    \n",
      "        if (objo - obj) / objo < 1e-5:\n",
      "            break\n",
      "    \n",
      "    return Z, M"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Call your kmeans function on the data\n",
      "\n",
      "#TODO: k_means_labels, k_means_cluster_centers = \n",
      "\n",
      "# plot the final result\n",
      "tut.plot_2d_clusters(X, k_means_labels, k_means_cluster_centers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**4)** K-means is often used to quantise various datasets, for instance, we can use it to \"compress\" images by clustering the RGB data into fewer colors. Open up the [kmeans-image](http://localhost:8888/notebooks/kmeans-image.ipynb) notebook to run this example (if you are running out of time, feel free to move on)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Gaussian Mixture Models\n",
      "[Gaussian mixture models](http://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model) (GMMs) can be viewed as a probabilistic generalisation of K-means (i.e. probabilistic cluster assignments) with the added ability to learn ellipsoidal clusters. In a GMM each observation is distributed according to a weighted sum of $K$ multivariate Gaussian distributions;\n",
      "\\begin{equation}\n",
      "   \\obsind_n \\sim \\sum^K_{k=1} \\wgtmix_k \\gausC{\\obsind_n}{\\gausmean_k,\n",
      "      \\gauscov_k}.\n",
      "\\end{equation}\n",
      "Here $\\wgtind = [\\wgtmix_1,\\ldots,\\wgtmix_k,\\ldots,\\wgtmix_K]\\transpose$ and $\\wgtmix_k \\in [0,1]$, with $\\sum_k \\wgtmix_k = 1$. $\\gauscov_k$ is a covariance matrix that describes the \"spread\" of uncertainty in the Gaussian distribution, which is usually ellipsoidal in shape. Here is an example of what a GMM may look like in two dimensions (we have plotted the 65% probability mass contours for the Gaussians):\n",
      "\n",
      "![2D GMM](clusters.png)\n",
      "\n",
      "The weights of each Gaussian have in this mixture have not been explicitly represented, but they are implicit in the amount of observations (darkness of the voxels) they are modelling.\n",
      "\n",
      ">The following is a brief explanation of a GMM, it is more complex than K-means, so do not worry if you don't understand it fully - a full grasp of GMMs is not required for the exercises.\n",
      "\n",
      "We need a way to explicitly assign observations to mixtures or clusters. The same latent variable, $\\olabind_n$, used in K-means is introduced here as an auxiliary variable for this purpose, by inducing the following conditional relationship;\n",
      "\\begin{equation}\n",
      "   \\probC{\\obsind_n}{\\olabind_n} =  \\prod^K_{k=1}\n",
      "      \\gausC{\\obsind_n}{\\gausmean_k, \\gauscov_k}^\\indic{\\olabind_n=k},\n",
      "\\end{equation}\n",
      "so given a cluster, $\\probC{\\obsind_n}{\\olabind_k=k} = \\gausC{\\obsind_n}\n",
      "{\\gausmean_k, \\gauscov_k}$. Now it can be seen that each cluster is\n",
      "modelled as a single Gaussian with a full covariance matrix. This auxiliary\n",
      "variable is itself distributed according to a Categorical distribution (same as a Multinomial distribution but with only one observation);\n",
      "\\begin{equation}\n",
      "   \\olabind_n \\sim \\categ{\\wgtind} \n",
      "   = \\prod^K_{k=1} \\wgtmix_k^\\indic{\\olabind_n = k}.\n",
      "\\end{equation}\n",
      "The joint distribution for this GMM can be written as,\n",
      "\\begin{equation}\n",
      "   \\prob{\\obsall, \\olaball} = \\prod^N_{n=1} \\categC{\\olabind_n}{\\wgtind} \n",
      "      \\prod^K_{k=1} \n",
      "      \\gausC{\\obsind_n}{\\gausmean_k, \\gauscov_k}^\\indic{\\olabind_n=k}.\n",
      "\\end{equation}\n",
      "\n",
      "Now we need an algorithm that can learn the labels, $\\olabind_n$, cluster\n",
      "parameters, $\\gausmean_k$ and $\\gauscov_k$, and mixture weights, $\\wgtind$. Such an algorithm can be derived by maximising the *log-likelihood* of the data, \n",
      "\\begin{equation}\n",
      "    \\log \\prob{\\obsall} = \\sum_{n=1}^N\n",
      "    \\log \\sum^K_{k=1} \\wgtmix_k \\gausC{\\obsind_n}{\\gausmean_k,\n",
      "      \\gauscov_k},\n",
      "\\end{equation}\n",
      "which is acheived by setting the partial derivative of this equation with respect to each parameter and the labels in turn to zero and solving for the parameters/labels.\n",
      "\n",
      "Firstly, maximising the log-likelihood with respect to $\\olabind_n$, yields;\n",
      "\\begin{equation}\n",
      "   \\probC{\\olabind_n = k}{\\obsind_n} = \n",
      "       \\frac{\\wgtmix_k \\gausC{\\obsind_n}{\\gausmean_k, \\gauscov_k}}\n",
      "       {\\sum_l \\wgtmix_l \\gausC{\\obsind_n}{\\gausmean_l, \\gauscov_l}},\n",
      "\\end{equation}\n",
      "This is known as the *expectation* step, since the labels are probabilistically assigned their expected value given the observations and cluster parameters. \n",
      "\n",
      "Next, the parameters can be found by maximising the log-likelihood \n",
      "with respect to each parameter; \n",
      "\\begin{align}\n",
      "   \\gausmean_k &= \\frac{\\sum_n \\probC{\\olabind_n=k}{\\obsind_n} \\obsind_n}\n",
      "      {\\sum_n \\probC{\\olabind_n=k}{\\obsind_n}}, \\\\\n",
      "   \\gauscov_k &= \\frac{1}{\\sum_n \\probC{\\olabind_n=k}{\\obsind_n}}\n",
      "      \\sum^N_{n=1} \\probC{\\olabind_n = k}{\\obsind_n} (\\obsind_n -\n",
      "      \\gausmean_k)(\\obsind_n - \\gausmean_k)\\transpose, \\\\\n",
      "   \\wgtmix_k &= \\sum^N_{n=1} \\frac{\\probC{\\olabind_n = k}{\\obsind_n}}\n",
      "      {\\sum_k \\probC{\\olabind_n = k}{\\obsind_n}}.\n",
      "\\end{align}\n",
      "This is called the *maximisation* step, because the value of the\n",
      "log-likelihood is maximised with respect to the parameters given the estimated latent variables. These two steps are iterated until the log-likelihood converges. This is known as the *expectation-maximisation* EM algorithm. For all intents and purposes it is the same algorithm used to learn K-means. \n",
      "\n",
      "Unfortunately this algorithm has a few drawbacks. Like K-means, it is only\n",
      "guaranteed to converge to a local maximum of the likelihood function. Also, the Gaussian cluster updates require a full $D \\times D$ covariance matrix inversion, which has an $\\bigO{D^3}$ computational cost. This can be circumvented by using diagonal covariance Gaussian clusters, or other distributions such as Multinomial, that have only $\\bigO{D}$ computational cost. Though some expressive power is lost since inter-dimensional correlation is not modelled.\n",
      "\n",
      "Another drawback is that this algorithm still cannot choose $K$. One way to\n",
      "allow the EM algorithm to choose $K$ is to include a penalty, or\n",
      "regulariser, for having too many parameters. In this way the maximum-likelihood fitting objective can be traded off against a model complexity penalty. Some popular penalties are the [Akaike information criterion](http://en.wikipedia.org/wiki/Akaike_information_criterion) and the [Bayesian information criterion](http://en.wikipedia.org/wiki/Bayesian_information_criterion). These criterion tend to under-penalise model complexity, and are sometimes computationally costly to calculate. Another way to choose $K$ is to use a fully Bayesian treatment, where we place prior distributions on the parameters (e.g. mean, covariance and weights), and then optimise the [log *marginal* likelihood](http://en.wikipedia.org/wiki/Marginal_likelihood) of the model, which also naturally incorporates a penalty for complexity. In the case of mixture models, the Bayesian learning algorithms can have very little additional computational cost compared to EM. For more information on EM algorithms for mixture models, see (Bishop, 2006) Chapters 9 and 10.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercises\n",
      "\n",
      "**5)** Run K-means on the following dataset with 3 clusters and visualise the results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load X from a dataset generation function from tututils\n",
      "X, truth = tut.load_2d_hard()\n",
      "\n",
      "#TODO: run K-means\n",
      "\n",
      "tut.plot_2d_clusters(X, labels, centres)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**6)** Now cluster the same data with a GMM, why do you think these results are so different?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.mixture import GMM\n",
      "\n",
      "# Hint, use covariance_type='full' option\n",
      "\n",
      "#TODO: run GMM\n",
      "\n",
      "# Plot\n",
      "tut.plot_2d_GMMs(X, labels, means, covs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**7)** Ok, now try K-means and GMMs with 6 clusters and plot the results, what happens?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Kmeans clustering\n",
      "\n",
      "#TODO: run K-means\n",
      "\n",
      "tut.plot_2d_clusters(X, labels_k, centres)\n",
      "\n",
      "# GMM clustering\n",
      "\n",
      "#TODO: run GMM\n",
      "\n",
      "# Plot\n",
      "tut.plot_2d_GMMs(X, labels_g, means, covs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see, if we don't have a reasonable idea of the number of clusters before we begin, we may not get great results. As mentioned before, there are clustering methods that can also infer the number of clusters, some of which include:\n",
      "* variational Bayes Gaussian mixture models (VBGMM) ([Attias, 1999](http://arxiv.org/pdf/1301.6676.pdf); Bishop, 2006)\n",
      "* Dirichlet process or infinite Gaussian mixture models ([Rasmussen, 1999](http://www.gatsby.ucl.ac.uk/~edward/pub/inf.mix.nips.99.pdf))\n",
      "* Spectral clustering ([von Luxburg, 2007](http://www.informatik.uni-hamburg.de/ML/contents/people/luxburg/publications/Luxburg07_tutorial.pdf))\n",
      "* [DBSCAN](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
      "\n",
      "**8)** If we have some kind of ground truth labels (as we do in the `truth` variable from ex. 5) we can get a measure of a clustering solutions validity from various measures. One of the more popular and robust measure is [Normalised Mutual Information](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html) (NMI). What is nice about this measure is that the *number of cluster labels do not have to match the number of ground truth labels* unlike many classification metrics. \n",
      "\n",
      "Run both K-means and the GMM for varying number of clusters (1 to 6), and plot their NMI vs. number of clusters.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import normalized_mutual_info_score\n",
      "\n",
      "Ks = range(1, 7)\n",
      "NMI_k = []\n",
      "NMI_g = []\n",
      "\n",
      "for k in Ks:\n",
      "    \n",
      "    # Kmeans clustering\n",
      "    \n",
      "    #TODO: run K-means\n",
      "    #TODO: get NMI\n",
      "    #TODO: append NMI, i.e. NMI_k.append(TODO)\n",
      "    \n",
      "    # GMM clustering\n",
      "    \n",
      "    #TODO: run GMM\n",
      "    #TODO: get NMI\n",
      "    #TODO: append NMI, i.e. NMI_g.append(TODO)\n",
      "\n",
      "#TODO: Plot both K-means NMI and GMM NMI vs. number of clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In reality, if we had all, or even some, of the labels like this it would more prudent to use a supervised classification method and the corresponding supervised classification performance metrics (like percent classification error, F-measure, etc). However clustering is still useful as a data exploration strategy, as well as a feature creation method for classification -- it is quite common to see kmeans with (very) large K being used to cluster inputs to a linear classifier to improve classification performance, among other strategies.\n",
      "\n",
      "As a final note, I would recommend against using the implementation of [Variational Bayes GMMs](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.VBGMM.html) and [Dirichlet Process GMMs](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.DPGMM.html) in scikit learn for now, I have never had much luck with them, and could not even get them to fit the above datasets. If you would like to test out these algorithms, I have a much more robust implementation in [libcluster](https://github.com/dsteinberg/libcluster). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Topic Models\n",
      "The purpose of topic modelling is to generally learn some low-dimensional representation of a large collection of textual documents, called a *corpus*. The representation can be used to summarize a corpus, or for retrieval of documents that are similar to a query document.\n",
      "\n",
      "Typically topic models are learned by analysing the distribution of words within a corpus, in particular how they cluster together within documents that have similar subjects. These clusters of words are referred to as \"topics\", and documents may comprise several topics in different amounts. These topics are essentially the low-dimensional representation of documents learned by topic models. \n",
      "\n",
      "## Latent Dirichlet Allocation\n",
      "One of the most well known topic models is [latent Dirichlet allocation](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) (Blei, 2003), also known as multinomial PCA (Buntine, 2002). \n",
      "\n",
      "Typically a corpus is broken down as follows:\n",
      "* There are $J$ documents in a corpus,\n",
      "* there are $N_j$ words in each document, $j$,\n",
      "* a word, $\\sobsind_{nj}$, is represented as an *index* into a vocabulary, i.e. $\\sobsind_{jn} \\in \\cbrac{1,\\ldots,D}$ where $D$ is the size of the vocabulary.\n",
      "\n",
      "So we can think of our whole dataset (corpus) being made up of sub-datasets (documents), $\\obsall = \\cbrac{\\obsall_j}^J_{j=1}$, which are collections of words in each document, $\\obsall_j = \\cbrac{\\sobsind_{jn}}^{N_j}_{n=1}$. This is called a *bag of words* model, because order of the words is assumed unimportant. This simplifying assumption is known as the exchangeability assumption. \n",
      "\n",
      "LDA goes on to model a corpus as follows:\n",
      "* Each document, $j$, is represented as a *low-dimensional* proportion of $K$ topics $$\\wgtind_j \\sim \\dir{\\dirall},$$ where $\\wgtind_j = \\cbrac{\\wgtmix_{j1}, \\ldots, \\wgtmix_{jK}}$ and $\\sum_k \\wgtmix_k = 1$.\n",
      "* Each word, $\\sobsind_{jn}$, is drawn from a *per-topic vocabulary*, represented as a categorical distribution, in proportion to the *per-document topic mixture* $$\\sobsind_{jn} \\sim \\sum^K_{k=1} \\wgtmix_{jk} \\categC{\\sobsind_{jn}}{\\mwgtind_k},$$ where $\\mwgtind_k$ are the parameters of the topic categorical distribution (which are also vector of weights that sum to one).\n",
      "\n",
      "This is very similar to the GMM we saw before, the differences being that we now have Categorical clusters as opposed to Gaussian clusters, and that we have mixture weights, $\\wgtmix_{jk}$, *specific to each document*. What is nice is that the clusters/topics are *shared* over all documents. This enables us to view these topic/cluster weights, $\\wgtind_j$ as a low-dimensional description of the original document, i.e. a mixture of topics!\n",
      "\n",
      "This is a Bayesian model and sometimes has a prior also placed on $\\mwgtind_k \\sim \\dir{\\dircall}$, which is called smoothed LDA.\n",
      "\n",
      "LDA can use fast sampling techniques, such as Gibbs sampling, or approximate marginal likelihood techniques, such as variational Bayes, for learning the model latent variables and hyper-parameters. Limitations have also been found with LDA. For example, it is not effective in choosing the number of topics ($K$), and the symmetric Dirichlet prior over topic weights, has been found to be too restrictive. Hierarchical Dirichlet processes (Teh, 2006) have been developed to overcome both of these issues (but is beyond the scope of this tutorial)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**9)** For this exercise we will run LDA on a fairly standard dataset comprising news articles from Reuters.\n",
      "\n",
      "For this exercise you'll need [this](https://pypi.python.org/pypi/lda) python LDA package. It can be installed via pip easily:\n",
      "    \n",
      "    $ sudo pip install lda  # you may need to use your distribution's version of pip (pip3, pip-python3, conda etc)\n",
      "\n",
      "Now, essentially follow the getting started tutorial in the [documentation](http://pythonhosted.org//lda/getting_started.html). It is worth noting that this dataset has undergone quite a bit of pre-processing, such as removing certain [\"stop\" words](http://en.wikipedia.org/wiki/Stop_words) so the resultant topics are not inundated with common and uninformative words likes \"and\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import lda\n",
      "import lda.datasets\n",
      "\n",
      "# Load the data\n",
      "X = lda.datasets.load_reuters()\n",
      "vocab = lda.datasets.load_reuters_vocab()\n",
      "\n",
      "# TODO: create the LDA object with 20 topics\n",
      "#model = \n",
      "\n",
      "# TODO: fit the LDA object to data\n",
      "\n",
      "# Print out a selection of topics\n",
      "topic_word = model.topic_word_  # model.components_ also works\n",
      "n_top_words = 8\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**10)** Try setting the number of topics found to different numbers, and see what impact there is on the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO, repeat above, but with a different number of topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bibliography\n",
      "\n",
      "* (Bishop, 2006) C. M. Bishop. Pattern Recognition and Machine Learning. Springer Science+Business Media, Cambridge, UK, 2006.\n",
      "\n",
      "* (Blei, 2003) D. M. Blei and M. I. Jordan. Modeling annotated data. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR \u201903, pages 127\u2013134, New York, NY, USA, 2003. ACM. ISBN 1-58113-646-3.\n",
      "\n",
      "* (Buntine, 2002) W. Buntine. Variational extensions to EM and multinomial PCA. Machine Learning: ECML 2002, pages 23\u201334, 2002.\n",
      "\n",
      "* (Teh, 2006) Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566\u20131581, 2006.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}