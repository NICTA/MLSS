{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Logistic Regression\n",
      "Logistic regression is a binary classfication method, so named due to its similarity with linear regression.\n",
      "\n",
      "The goal of binary classification is to predict the label $y_i \\in \\{-1,1\\}$ given a set of features $\\boldsymbol{x}_i$ for each row, $i$, of data. Logistic regression assumes the probability of $y_i=1$ given $\\boldsymbol{x_i}$ is a (logistic) function of a linear combination of the features $\\boldsymbol{x_i}$ and independent of the labels or features of other rows of data. The logistic function $\\theta(s) = \\frac{e^s}{e^s+1}$ maps the weighted features to $[0,1]$ to allow it to model a probability. Training logistic regression corresponds to learning the weights $\\boldsymbol{w}$ to maximize the likelyhood function:\n",
      "\n",
      "\\begin{equation}\n",
      "P(y_1...y_n|\\boldsymbol{x}_1...\\boldsymbol{x}_n,\\boldsymbol{w}) = \\prod_{i=1}^n \\theta(y_i\\boldsymbol{w}^T\\boldsymbol{x}_i)\n",
      "\\end{equation}\n",
      "\n",
      "Once we have these weights, we can predict the probability $p_j$ that a new observation with features $\\boldsymbol{x_j}$ belongs to class $+1$. We convert this to a classification by choosing a threshold for $p$ ie $1/2$ "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run Logistic regression on some a simple 2D data problem\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what is the decicion boundary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run logistic regression on the titanic data\n",
      "\n",
      "# Interpret coefficients"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise: Suppose that we have a data set that is linearly seperable. What happens to the weights $w$ when we run linear regression?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Implement Logistic Regression (Optional)\n",
      "Maximizing the likelyhood $P(y_1...y_n|\\boldsymbol{x}_1...\\boldsymbol{x}_n,\\boldsymbol{w})$ is equivelent to minimizing the negative log-likelyhood: \n",
      "\\begin{equation}\n",
      "\\boldsymbol{w}^* = argmin_{\\boldsymbol{w}}\\left( -\\log\\left(\\prod_{i=1}^n \\theta(y_i\\boldsymbol{w}^T\\boldsymbol{x}_i)\\right)\\right)\n",
      "= argmin_{\\boldsymbol{w}}\\left( \\sum_{i=1}^n \\ln(1+e^{-y_i\\boldsymbol{w}^T\\boldsymbol{x}_i})\\right)\n",
      "\\end{equation}\n",
      "\n",
      "Gradient decent or with python's optimize library"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}