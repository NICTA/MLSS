{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "%load_ext autoreload\n",
      "%autoreload\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as pl\n",
      "import util\n",
      "#print(numpy.__version__)\n",
      "X, Y = util.load_data() # passenger_class, is_female, sibsp, parch, fare, embarked (categorical 0-3)\n",
      "X_demean = X - np.mean(X, axis=0)\n",
      "X_unitsd = X_demean/(np.std(X_demean,axis=0))\n",
      "X_whiten = np.dot(X_demean, util.whitening_matrix(X_demean))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n",
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Naive Bayes\n",
      "\n",
      "Suppose we want to build a model to predict the probability of survival on the Titanic based on just two categorical features, a persons class (1,2 or 3) and their sex (1=female,0=male). An obvious approach would be to create a category for each combination of our features (female 1st, female 2nd ... male 3rd) and calculate the proportion who survived in each as an estimate for the survival probability. For each observation in our test data - we simply look up the survival rate in the corresponding category.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "combinations = [(i,j) for i in [0,1] for j in [1,2,3]]\n",
      "for c in combinations:\n",
      "    match = np.where((X[:,0] == c[1]) * (X[:,1] == c[0]))[0]\n",
      "    print(c,sum(Y[match])/len(match))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0, 1) 0.368852459016\n",
        "(0, 2) 0.157407407407\n",
        "(0, 3) 0.135446685879\n",
        "(1, 1) 0.968085106383\n",
        "(1, 2) 0.921052631579\n",
        "(1, 3) 0.5\n"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Question: Why will this approach not work in general? What happens as we increase the number of features or the number of values each feature can take?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Naive Bayes assumes that the data was generated by a model where all the features are independent on one-another given the class label.\n",
      "\n",
      "\\begin{equation}\n",
      "P(y|\\boldsymbol{x}) \\propto P(y)\\prod_{j=1}^D P(x_j|y)\n",
      "\\end{equation}\n",
      "\n",
      "Training a Naive Bayes model corresponds to learning maximum likelyhood estimates for $P(y)$ and $P(x_j|y)$ for each feature $j$ The maximum likelyhood estimate for $P(y=c)$ is simply the proportion of times class $c$ occures in the training set. Estimating $P(x_j|y)$ requires us to assume a model - for example Gaussian for continous features, Bernoulli for binary features, etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Implement a Naive Bayes model for the Titanic data set using passenger_class and is_female as features\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run Naive Bayes from Sci-Kit Learn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Question: How reasonalbe is the Naive Bayes assumption in this case. What about if we included variables 'Age' and 'Parch'?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Logistic Regression\n",
      "Logistic regression is a binary classfication method, so named due to its similarity with linear regression.\n",
      "\n",
      "The goal of binary classification is to predict the label $y_i \\in \\{-1,1\\}$ given a set of features $\\boldsymbol{x}_i$ for each row, $i$, of data. Logistic regression assumes the probability of $y_i=1$ given $\\boldsymbol{x_i}$ is a (logistic) function of a linear combination of the features $\\boldsymbol{x_i}$ and independent of the labels or features of other rows of data. The logistic function $\\theta(s) = \\frac{e^s}{e^s+1}$ maps the weighted features to $[0,1]$ to allow it to model a probability. Training logistic regression corresponds to learning the weights $\\boldsymbol{w}$ to maximize the likelyhood function:\n",
      "\n",
      "\\begin{equation}\n",
      "P(y_1...y_n|\\boldsymbol{x}_1...\\boldsymbol{x}_n,\\boldsymbol{w}) = \\prod_{i=1}^n \\theta(y_i\\boldsymbol{w}^T\\boldsymbol{x}_i)\n",
      "\\end{equation}\n",
      "\n",
      "Once we have these weights, we can predict the probability that a new observation belongs to each class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run Logistic regression on some a simple 2D data problem\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what is the decicion boundary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run logistic regression on the titanic data\n",
      "\n",
      "# Interpret coefficients"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise: Suppose that we have a data set that is linearly seperable. What happens to the weights $w$ when we run linear regression?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Implement Logistic Regression (Optional)\n",
      "Maximizing the likelyhood $P(y_1...y_n|\\boldsymbol{x}_1...\\boldsymbol{x}_n,\\boldsymbol{w})$ is equivelent to minimizing the negative log-likelyhood: \n",
      "\\begin{equation}\n",
      "\\boldsymbol{w}^* = argmin_{\\boldsymbol{w}}\\left( -\\log\\left(\\prod_{i=1}^n \\theta(y_i\\boldsymbol{w}^T\\boldsymbol{x}_i)\\right)\\right)\n",
      "= argmin_{\\boldsymbol{w}}\\left( \\sum_{i=1}^n \\ln(1+e^{-y_i\\boldsymbol{w}^T\\boldsymbol{x}_i})\\right)\n",
      "\\end{equation}\n",
      "\n",
      "Gradient decent or with python's optimize library"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}