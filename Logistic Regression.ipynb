{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "%load_ext autoreload\n",
      "%autoreload\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as pl\n",
      "import util\n",
      "from scipy.stats import itemfreq\n",
      "#print(numpy.__version__)\n",
      "X, Y = util.load_data() # passenger_class, is_female, sibsp, parch, fare, embarked (categorical 0-3)\n",
      "X_demean = X - np.mean(X, axis=0)\n",
      "X_unitsd = X_demean/(np.std(X_demean,axis=0))\n",
      "X_whiten = np.dot(X_demean, util.whitening_matrix(X_demean))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n",
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Naive Bayes\n",
      "\n",
      "Suppose we want to build a model to predict the probability of survival on the Titanic based on just two categorical features, a persons class (1,2 or 3) and their sex (1=female,0=male). An obvious approach would be to create a category for each combination of our features (female 1st, female 2nd ... male 3rd) and calculate the proportion who survived in each as an estimate for the survival probability. For each observation in our test data - we simply look up the survival rate in the corresponding category.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "combinations = [(i,j) for i in [1,2,3] for j in [0,1]]\n",
      "for c in combinations:\n",
      "    match = np.where((X[:,0] == c[0]) * (X[:,1] == c[1]))[0]\n",
      "    print(c,sum(Y[match])/len(match))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1, 0) 0.368852459016\n",
        "(1, 1) 0.968085106383\n",
        "(2, 0) 0.157407407407\n",
        "(2, 1) 0.921052631579\n",
        "(3, 0) 0.135446685879\n",
        "(3, 1) 0.5\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Question: Why will this approach not work in general? What happens as we increase the number of features or the number of values each feature can take?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Naive Bayes assumes that the data was generated by a model where all the features are independent on one-another given the class label.\n",
      "\n",
      "\\begin{equation}\n",
      "P(y|\\boldsymbol{x}) \\propto P(y)\\prod_{j=1}^D P(x_j|y)\n",
      "\\end{equation}\n",
      "\n",
      "Training a Naive Bayes model corresponds to learning maximum likelyhood estimates for $P(y)$ and $P(x_j|y)$ for each feature $j$ The maximum likelyhood estimate for $P(y=c)$ is simply the proportion of times class $c$ occures in the training set. \n",
      "\n",
      "To estimate $P(x_j|y)$ we can use a parametric model (assume a particular distributional form) or do some form of density estimation (ie kernal density estimation)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Excersize: Implement a Naive Bayes model for the Titanic data set using passenger_class and is_female as features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a function that may be useful\n",
      "def proportions(array):\n",
      "    \"\"\" returns an map from each unique value in the input array to the proportion of times that value occures \"\"\"\n",
      "    prop = itemfreq(array)\n",
      "    prop[:,1] = prop[:,1]/sum(prop,axis=0)[1]\n",
      "    return dict(prop)\n",
      "\n",
      "class Naive_Bayes:\n",
      "    def train(self,X,Y):\n",
      "        \"\"\" trains the model with features X and labels Y \"\"\"\n",
      "        # 1) Estimate P(Y=1)\n",
      "        self.py = sum(Y)/len(Y)\n",
      "\n",
      "        # 2) For each feature, x, estimate P(x|y=1) and P(x|y=0)\n",
      "        self.survived = X[np.where(Y==1)[0],:] # the features of those who survived\n",
      "        self.died  = X[np.where(Y==0)[0],:] # the features for those who died\n",
      "\n",
      "        # estimate P(gender|survived) - return a map from gender value to probability\n",
      "        self.gs = proportions(survived[:,1])\n",
      "\n",
      "        # estimate P(class|survived) - return a map from class to probability\n",
      "        self.cs = proportions(survived[:,0]) \n",
      "\n",
      "        # estimate P(gender|died)-return a map from gender value to probability\n",
      "        self.gd = proportions(died[:,1])\n",
      "\n",
      "        # estimate P(class|died) - return a map from class to probability\n",
      "        self.cd = proportions(died[:,0])\n",
      "    \n",
      "    def predict(self,sex,p_class):\n",
      "        \"\"\" outputs the probability of survival for a given class and gender \"\"\"\n",
      "        # caclulate unormalized P(y = 1|sex,p_class) as P(y=1)P(sex|y=1)P(p_class|y=1) \n",
      "        ps = self.py*self.gs[sex]*self.cs[p_class]\n",
      "\n",
      "        # calculate unormalized P(y = 0|sex,p_class) as P(y=0)P(sex|y=0)P(p_class|y=0)\n",
      "        pd = self.py*self.gd[sex]*self.cd[p_class]\n",
      "\n",
      "        # calculates the survival ratio as ps/pd and the normalized probability  from the ratio\n",
      "        r = ps/pd\n",
      "        psn = r/(1+r)\n",
      "        return psn\n",
      "\n",
      "# run the model\n",
      "model = Naive_Bayes()\n",
      "model.train(X,Y)\n",
      "for p_class,sex in combinations:\n",
      "    print((p_class,sex),model.predict(sex,p_class))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1, 0) 0.505020642817\n",
        "(1, 1) 0.92647713763\n",
        "(2, 0) 0.349929783957\n",
        "(2, 1) 0.869252066223\n",
        "(3, 0) 0.161066331522\n",
        "(3, 1) 0.703369342447\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise: Compare these predictions with those just based on the proportion of survivals. How true is the Naive Bayes assumption for this case?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Question: How does the number of parameters to be learnt scale with the number of features for Naive Bayes?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exersise: Run Naive Bayes from Sci-Kit Learn using the same features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "# Sklearn doesn't have a model that expects categorical data. \n",
      "# We need to first encode our (p_class, sex) to (is_first,is_second,is_third,is_female,is_male)\n",
      "\n",
      "# use preprocessing.OneHotEncoder to create a new dataset X2 that is the transformation of the first 2 columns of X\n",
      "\n",
      "enc = OneHotEncoder()\n",
      "X2 = enc.fit(X[:,0:2]).transform(X[:,0:2]).toarray()\n",
      "\n",
      "# fit a Multinommial Naive Bayes Model\n",
      "\n",
      "nb = MultinomialNB()\n",
      "nb.fit(X2,Y)\n",
      "\n",
      "# transforms our combinations to the one-hot encoding\n",
      "c = enc.transform(np.asarray(combinations)).toarray()\n",
      "\n",
      "# gets predictions for each combination\n",
      "predictions = nb.predict_proba(c)\n",
      "\n",
      "# prints your predictions in the same format as previous models\n",
      "for i in range(len(c)):\n",
      "    print(combinations[i],predictions[i][1])\n",
      "#hmmm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1, 0) 0.387749300694\n",
        "(1, 1) 0.885130809184\n",
        "(2, 0) 0.251628583642\n",
        "(2, 1) 0.803573287808\n",
        "(3, 0) 0.107512893467\n",
        "(3, 1) 0.594433255849\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Logistic Regression\n",
      "Logistic regression is a binary classfication method, so named due to its similarity with linear regression.\n",
      "\n",
      "The goal of binary classification is to predict the label $y_i \\in \\{-1,1\\}$ given a set of features $\\boldsymbol{x}_i$ for each row, $i$, of data. Logistic regression assumes the probability of $y_i=1$ given $\\boldsymbol{x_i}$ is a (logistic) function of a linear combination of the features $\\boldsymbol{x_i}$ and independent of the labels or features of other rows of data. The logistic function $\\theta(s) = \\frac{e^s}{e^s+1}$ maps the weighted features to $[0,1]$ to allow it to model a probability. Training logistic regression corresponds to learning the weights $\\boldsymbol{w}$ to maximize the likelyhood function:\n",
      "\n",
      "\\begin{equation}\n",
      "P(y_1...y_n|\\boldsymbol{x}_1...\\boldsymbol{x}_n,\\boldsymbol{w}) = \\prod_{i=1}^n \\theta(y_i\\boldsymbol{w}^T\\boldsymbol{x}_i)\n",
      "\\end{equation}\n",
      "\n",
      "Once we have these weights, we can predict the probability that a new observation belongs to each class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run Logistic regression on some a simple 2D data problem\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what is the decicion boundary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run logistic regression on the titanic data\n",
      "\n",
      "# Interpret coefficients"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise: Suppose that we have a data set that is linearly seperable. What happens to the weights $w$ when we run linear regression?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Implement Logistic Regression (Optional)\n",
      "Maximizing the likelyhood $P(y_1...y_n|\\boldsymbol{x}_1...\\boldsymbol{x}_n,\\boldsymbol{w})$ is equivelent to minimizing the negative log-likelyhood: \n",
      "\\begin{equation}\n",
      "\\boldsymbol{w}^* = argmin_{\\boldsymbol{w}}\\left( -\\log\\left(\\prod_{i=1}^n \\theta(y_i\\boldsymbol{w}^T\\boldsymbol{x}_i)\\right)\\right)\n",
      "= argmin_{\\boldsymbol{w}}\\left( \\sum_{i=1}^n \\ln(1+e^{-y_i\\boldsymbol{w}^T\\boldsymbol{x}_i})\\right)\n",
      "\\end{equation}\n",
      "\n",
      "Gradient decent or with python's optimize library"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}