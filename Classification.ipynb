{
 "metadata": {
  "name": "",
  "signature": "sha256:c325b755ed07bf053de1aeb918c61c786b83b4592cbdc8da48fe33993b078ad8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as pl\n",
      "import util\n",
      "X, Y = util.load_data()\n",
      "X_demean = X - np.mean(X, axis=0)\n",
      "X_unitsd = X_demean/(np.std(X_demean,axis=0))\n",
      "X_whiten = np.dot(X_demean, util.whitening_matrix(X_demean))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Classification#\n",
      "\n",
      "here is some introduction\n",
      "\n",
      "##Preprocessing Data##\n",
      "\n",
      "Some classification methods perform better when..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "util.illustrate_preprocessing()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'module' object has no attribute 'illustrate_preprocessing'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-19-1908d1538b1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0millustrate_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'illustrate_preprocessing'"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Naive Bayes##\n",
      "\n",
      "text text text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Logistic Regression##\n",
      "\n",
      "text text text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Support Vector Machines##\n",
      "\n",
      "Support Vector machines are a classification technique which attempts to separate classes of data with a plane. SVMs select the plane based on the following two intuitions:\n",
      "\n",
      "1. With two classes A and B, try to put all the A's on one side of the plane and the B's on the other\n",
      "2. Planes which try to maximise the gap between the plane and the datapoints give the most 'room for error', and so might generalise better to new data.\n",
      "\n",
      "The size of the gap between the two classes of points and the plane is known as the *margin*, whilst the points that lie on this margin are the *support vectors*. These are illustrated in the figure below.\n",
      "\n",
      "\n",
      "###Beyond Linear classification###\n",
      "In real problems, it may not be possible to draw a straight line between the classes (or more technciallly, the data are not 'linearly seperable').\n",
      "\n",
      "\n",
      "###Exercises###\n",
      "1. Come up with a dataset to break a linear SVM, without mixing the two categories together. Can you build one to break a SVM with RBF kernel in the same way?\n",
      "\n",
      "2. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    }
   ],
   "metadata": {}
  }
 ]
}